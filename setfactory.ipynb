{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from demografi import mergefunc\n",
    "from demografi import cleanfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#File paths to standardized datasets - manual addition of more datasets from project updates is possible,\n",
    "#but must be accompanied by an updated name array that is used to name additional variables from each dataset, e.g. \"event_parish_1787s\"\n",
    "#Example: Addition of source 11: PR Burials can be done by adding the corresponding .csv file to the end of the 'file_paths' list and\n",
    "#copying this code snippet at the end of this code cell: name_array.append('NAME_YOU_CHOOSE')\n",
    "\n",
    "#OBS: datasets in 'file_paths' must be in correct numerical order according to the link-lives source enumeration. See: Readme.me\n",
    "\n",
    "\n",
    "file_paths = ['C:/Users/juliu/Data/Kilder2/census1787s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1801s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1834s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1840s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1845s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1850s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1860s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1880s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1885s.csv',\n",
    " 'C:/Users/juliu/Data/Kilder2/census1901s.csv']\n",
    "\n",
    "CBPt = 'C:/Users/juliu/Data/Kilder1/CBPt.csv' #Copenhagen Burial Protocols. Standardized version can be used instead\n",
    "lifecourses = 'C:/Users/juliu/Data/life_courses.csv' #The life-courses set, containing life-course id's\n",
    "\n",
    "name_array = [file_path[34:39] for file_path in file_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the paths\n",
    "def assert_paths_exist(paths): \n",
    "    if isinstance(paths, str):  \n",
    "        paths = [paths]  \n",
    "    for path in paths:\n",
    "        assert os.path.exists(path), f\"Path does not exist: {path}\"\n",
    "\n",
    "assert_paths_exist(file_paths)\n",
    "assert_paths_exist(CBPt)\n",
    "assert_paths_exist(lifecourses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning iterator using the cleanfunc function from the .py file\n",
    "census_datasets = []\n",
    "for k, v in zip(file_paths, name_array):\n",
    "    kilde = cleanfunc(k, v)\n",
    "    census_datasets.append(kilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying and separating the Copenhagen Burial Protocol observations in the life-courses dataset\n",
    "\n",
    "lifecourses = pd.read_csv(lifecourses)\n",
    "\n",
    "#Splitting the data sources and observation i.d.'s into separate columns in order to operate on them\n",
    "lifecourses2 = lifecourses\n",
    "\n",
    "lifecourses2[['source0', 'source1', 'source2', 'source3', 'source4', 'source5', 'source6',\n",
    "               'source7', 'source8', 'source9', 'source10', 'source11', 'source12']]  = lifecourses2.source_ids.str.split(',', expand=True).astype('Int64')\n",
    "\n",
    "lifecourses2[['pa_id0', 'pa_id1', 'pa_id2', 'pa_id3', 'pa_id4', 'pa_id5', 'pa_id6', 'pa_id7',\n",
    "              'pa_id8', 'pa_id9', 'pa_id10', 'pa_id11', 'pa_id12']] = lifecourses2.pa_ids.str.split(',', expand=True).astype('str')\n",
    "\n",
    "\n",
    "#Identifying and separating out the observations present in the Copenhagen Burial Protocols\n",
    "I = lifecourses2.source_ids.str.contains('10')\n",
    "lifecourses2.loc[I]\n",
    "lifecourses2 = lifecourses2.loc[I]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the information in the (currently 9) standardized source datasets that contain CBP observations\n",
    "#OBS: datasets must be in correct numerical order according to the link-lives source enumeration. See: Readme.me\n",
    "merged_datasets = []\n",
    "for i, census in enumerate(census_datasets):\n",
    "    merged_dataset = mergefunc(lifecourses2, census, i)\n",
    "    merged_datasets.append(merged_dataset)\n",
    "\n",
    "#Merging the source datasets into 1\n",
    "concatenated_dataset = pd.concat(merged_datasets)\n",
    "\n",
    "print(concatenated_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the merged dataset\n",
    "\n",
    "#Ensuring that no information is lost\n",
    "aggregation_functions = {}\n",
    "for column in concatenated_dataset.columns:\n",
    "    aggregation_functions[column] = 'first' # Keep the first value of each column\n",
    "    \n",
    "concatenated_dataset_grouped = concatenated_dataset.groupby(['life_course_id']).aggregate(aggregation_functions)\n",
    " \n",
    "#Removing unneeded variables\n",
    "cleanedcensus0 = concatenated_dataset_grouped.drop(['pa_ids', 'link_ids', 'source0', 'source1', 'source2', 'source3', 'source4', \n",
    "'source5', 'source6', 'source7', 'source8', 'source9', 'source10', 'source11', 'source12', 'pa_id0', 'pa_id1', 'pa_id2', 'pa_id3', 'pa_id4',\n",
    "'pa_id5', 'pa_id6', 'pa_id7', 'pa_id8', 'pa_id9', 'pa_id10', 'pa_id11', 'pa_id12', 'pa_id'], axis=1)\n",
    "cleanedcensus0 = cleanedcensus0.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing, cleaning, and adding transcribed version of the Copenhagen Burial Protocols\n",
    "\n",
    "CBPt = pd.read_csv(CBPt)\n",
    "\n",
    "CBPt['pa_id'] = CBPt['pa_id'].astype(str)\n",
    "\n",
    "CBPtmerg = mergefunc(lifecourses2, CBPt, 10)\n",
    "\n",
    "CBPtmerg = CBPtmerg.drop(['pa_ids', 'link_ids', 'source_ids', 'source0', 'source1', 'source2', 'source3', 'source4', \n",
    "'source5', 'source6', 'source7', 'source8', 'source9', 'source10', 'source11', 'source12', 'pa_id0', 'pa_id1', 'pa_id2', 'pa_id3', 'pa_id4',\n",
    "'pa_id5', 'pa_id6', 'pa_id7', 'pa_id8', 'pa_id9', 'pa_id10', 'pa_id11', 'pa_id12', 'pa_id', 'id'], axis=1)\n",
    "\n",
    "cleanedcensus1 = cleanedcensus0.set_index('life_course_id')\n",
    "CBPtmerg1 = CBPtmerg.set_index('life_course_id')\n",
    "\n",
    "finaldata = cleanedcensus1.merge(CBPtmerg1, on='life_course_id', how='left')\n",
    "\n",
    "finaldata = finaldata.rename(columns={'n_sources_x':'n_sources', 'sex_x':'sex'})\n",
    "\n",
    "finaldata = finaldata.drop(['n_sources_y', 'sex_y'], axis=1)\n",
    "\n",
    "finaldata = finaldata.drop(['firstnames', 'lastname', 'birthname', 'number', 'dateOfBirth', 'yearOfBirth', 'street_unique'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldata = finaldata.drop_duplicates(subset=['name_cl', 'chapel','dateOfDeath']) # Dropping duplicates wrongly identified as unique by life_course_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating .csv file in local directory\n",
    "finaldata.to_csv('finaldata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
